{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e8ad954-c9d1-48b2-a909-6e6eaac83c6f",
   "metadata": {},
   "source": [
    "# Unit 4: Matrices and Linearization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835796b-7476-4483-a667-7f3666aba288",
   "metadata": {},
   "source": [
    "## Consider the figure below where we show $\\vec{v}$ before rotation and $\\vec{v}$ after rotation counterclockwise by an angle $\\theta$.\n",
    "\n",
    "![Tangent Plane](img/rotation.png)\n",
    "\n",
    "### In the before picture, we have\n",
    "## $$ \\vec{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} $$\n",
    "## $$ \\vec{a} = \\begin{pmatrix} v_1 \\\\ 0 \\end{pmatrix} $$\n",
    "## $$ \\vec{b} = \\begin{pmatrix} 0 \\\\ v_2 \\end{pmatrix} $$\n",
    "\n",
    "### In the after picture, we can write the vectors $\\vec{c}$ and $\\vec{d}$ using the computations we did in the warm-up problems. Namely, vector $\\vec{c}$ is found by rotating the horizontal vector $\\vec{a}$ counterclockwise by an angle $\\theta$. So\n",
    "## $$ \\vec{c} = \\begin{pmatrix} v_1 \\cos \\theta \\\\ v_1 \\sin \\theta \\end{pmatrix} $$\n",
    "### Similarly, $\\vec{d}$ is found by rotating the vertical vector $\\vec{b}$ counterclockwise by an angle $\\theta$.\n",
    "## $$ \\vec{d} = \\begin{pmatrix} - v_2 \\sin \\theta \\\\ v_2 \\cos \\theta \\end{pmatrix} $$\n",
    "### From here, we can use vector addition to find that $\\vec{w}$ is given by\n",
    "## $$ \\vec{w} = \\vec{c} + \\vec{d} = \\begin{pmatrix} v_1 \\cos \\theta - v_2 \\sin \\theta \\\\ v_1 \\sin \\theta + v_2 \\cos \\theta \\end{pmatrix} $$\n",
    "## $$ \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix} = \\begin{pmatrix} \\cos \\theta & - \\sin \\theta \\\\ \\sin \\theta & \\cos \\theta  \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa42a4a-52a0-49d5-8273-42734f01a9f4",
   "metadata": {},
   "source": [
    "## What Are Matrices?\n",
    "### Key point: A matrix is a succinct way to express a linear relationship.\n",
    "#### Express Linear Relationship Using Matrix Multiplication\n",
    "\n",
    "#### The linear relationship\n",
    "### $$ u_1 = 2 x_1 + 3 x_2 + 3 x_3 $$\n",
    "### $$ u_2 = 2 x_1 + 4 x_2 + 5 x_3 $$\n",
    "### $$ u_3 = 1 x_1 + 1 x_2 + 2 x_3 $$\n",
    "#### can be expressed as a matrix product:\n",
    "### $$ \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix} = \\begin{pmatrix} 2 & 3 & 3 \\\\ 2 & 4 & 5 \\\\ 1 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} $$\n",
    "#### This allows us to simply write $\\vec{u} = A \\vec{x}$. In words, the equation $\\vec{u} = A \\vec{x}$ expresses the fact that the components of $\\vec{u}$ are a linear function of the components of $\\vec{x}$.\n",
    "#### This works because of the definition of matrix multiplication.\n",
    "\n",
    "### Linear Relationship\n",
    "#### By â€œlinear relationship\" we mean an equation where all the terms are linear, that is, a variable times a constant (no powers, exponentials, etc). Matrices were invented in order to efficiently manage linear relationships, which are common throughout science and engineering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017529f-d2d8-4f23-8767-e487ab1257c8",
   "metadata": {},
   "source": [
    "## If\n",
    "## $$ \\vec{z} = A \\vec{y} $$\n",
    "## And\n",
    "## $$ \\vec{y} = B \\vec{x} $$\n",
    "## Then\n",
    "## $$ \\vec{z} = A B \\vec{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7155811-6585-45fb-b85a-6145f27bcdb8",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "### The $n \\times n$ identity matrix $I_n$, or just $I$, has the property that $I X = X$ for any matrix (or vector) $X$ with $n$ rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa7cbb-e184-45c2-b5fe-012c3d064db6",
   "metadata": {},
   "source": [
    "## if $A$ is the matrix that rotates a vector by $\\pi/3$ counter-clockwise, and $B$ is the matrix that rotates a vector by $\\pi/6$, then $A B$ is the matrix that rotates a vector by $\\pi/2$.\n",
    "## Matrix multiplication obeys the associative law $A (B C) = (A B) C$. But we do not have the commutative law, since in general $A B \\neq B A$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4f91f-1333-4d1d-b88e-8ef5d7e6676e",
   "metadata": {},
   "source": [
    "## The Inverse Matrix (2x2)\n",
    "### If \n",
    "## $$A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$$ \n",
    "### then the inverse of $A$ is \n",
    "## $$ A^{-1} = \\frac{1}{a d - b c} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix} $$\n",
    "\n",
    "### The most important property of $A^{-1}$ is ***Inverse Matrix Property***\n",
    "## $$ A A^{-1} = A^{-1} A = I $$\n",
    "\n",
    "### Imagine we want to solve for $\\vec{x}$ in the equation $A \\vec{x} = \\vec{b}$. We can multiply both sides by $A^{-1}$ and simplify:\n",
    "## $$ A^{-1} A \\vec{x} = A^{-1} \\vec{b} $$\n",
    "## $$ \\vec{x} = A^{-1} \\vec{b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ccd16f-3c3c-4943-a11c-29c560dccfad",
   "metadata": {},
   "source": [
    "## Determinants in 2x2\n",
    "### For a 2x2 matrix \n",
    "## $$A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$$ \n",
    "### the number \n",
    "## $$\\det(A) = a d - b c$$\n",
    "### is called the determinant of $A$. We have already seen this expression show up in the formula for $A^{-1}$:\n",
    "## $$ A^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be8bce-8b87-4eff-92c7-03752680b288",
   "metadata": {},
   "source": [
    "## Application to Area\n",
    "\n",
    "### The determinant also comes up in a completely separate problem. Given two points/vectors $\\vec{A} = \\begin{pmatrix} a_1 \\\\ a_2 \\end{pmatrix} $ and $\\vec{B} = \\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix} $ in the plane, what is the area of the parallelogram $P$ whose sides are described by $\\vec{A}$ and $\\vec{B}$ ?\n",
    "\n",
    "![Tangent Plane](img/det.png)\n",
    "\n",
    "### It can be shown that\n",
    "## $$ area(P) = \\left| \\det{ \\begin{pmatrix} a_1 & a_2 \\\\ b_1 & b_2 \\end{pmatrix} } \\right| = \\left| a_1 b_2 - a_2 b_1 \\right| $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b22c3-75e7-4875-aea6-08ea3a33a56b",
   "metadata": {},
   "source": [
    "## Definition 7.1 \n",
    "\n",
    "### In mathematics, the term ***intersection*** means \"common part.\" For example, the intersection of $P_1$ and $P_2$ is the set of points that belong to both $P_1$  and $P_2$. This is sometimes written as $P_1 \\cap P_2$.\n",
    "\n",
    "#### Most of the time, the intersection of two planes is a line. And the intersection of a line with a third plane is a single point. In other words, the solution to the 3x3 linear system lies at the intersection of the three planes.\n",
    "\n",
    "#### It could happen that the third plane  is parallel to the line where  and  intersect. This leads to two possibilities.\n",
    "\n",
    "#### One possibility is that the third plane completely contains the line common to  and . In this case, there are infinitely many solutions.\n",
    "\n",
    "## Definition 8.1 \n",
    "### A linear system that has infinitely many solutions is called ***under-determined***. This means there are not enough equations to determine a unique solution, or that some equations are redundant.\n",
    "\n",
    "#### A second possibility is that the third plane does not pass through the line common to  and . In this case, we have no possible solutions.\n",
    "\n",
    "## Definition 8.2 \n",
    "\n",
    "### A linear system that has no possible solutions is called ***over-determined***. This means the equations contradict one another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65517c-7246-4f60-a770-374b0525bc0d",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "### The key takeaway is that even if we have $n$ equations and $n$ unknowns, the system could be under-determined or over-determined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70272b0-c28e-4447-935b-5ba9ced7839d",
   "metadata": {},
   "source": [
    "## To find the determinant of a 3x3 matrix, we have the following formula.\n",
    "# $$ \\begin{vmatrix} a_1 & a_2 & a_3 \\\\ b_1 & b_2 & b_3 \\\\ c_1 & c_2 & c_3 \\end{vmatrix} = a_1 \\begin{vmatrix} b_2 & b_3 \\\\ c_2 & c_3 \\end{vmatrix} - a_2 \\begin{vmatrix} b_1 & b_3 \\\\ c_1 & c_3 \\end{vmatrix} + a_3 \\begin{vmatrix} b_1 & b_2 \\\\ c_1 & c_2 \\end{vmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698cd19-b745-475b-a5cc-29a49bdb80a0",
   "metadata": {},
   "source": [
    "## Linearization\n",
    "\n",
    "### This process of approximating a transformation with a linear function is called ***linearization***. Linearizing a function at a point means computing the matrix that tells you how the output changes when the inputs change near that point.\n",
    "\n",
    "### Linearization Steps\n",
    "\n",
    "### - Step 1. Find the equations that describes the relationship between input variables and output variables. Decide what should be the base point $(x_0, y_0)$ (assuming there are two input variables $x$ and $y$).\n",
    "### - Step 2. For each output variable $P$, do a linear approximation of $P(x_0 + \\Delta x, y_0 + \\Delta y)$. You will need to compute the partial derivatives evaluated at $(x_0, y_0)$. Repeat for the other output variables.\n",
    "### - Step 3. Find the matrix that captures the linear approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2347a-b52a-4e92-a7a1-db5274e8647f",
   "metadata": {},
   "source": [
    "### Example: \n",
    "\n",
    "## $$ A = (x + y)2 - 2 y $$\n",
    "## $$ B = (x - y) ^2 $$\n",
    "## $$ A_x = 2(x + y), A_y = 2(x + y) - 2 $$\n",
    "## $$ B_x = 2(x - y), B_y = 2(y - x) $$\n",
    "## $$ \\text{Near (0, 1):} $$\n",
    "## $$ A_x = 2, A_y = 0, B_x = -2, B_y = 2 $$\n",
    "## $$ \\Delta A = A_x \\Delta x + A_y \\Delta y = 2 \\Delta x $$\n",
    "## $$ \\Delta B = B_x \\Delta x + B_y \\Delta y = -2 \\Delta x + 2 \\Delta y $$\n",
    "## $$ \\begin{pmatrix} \\Delta A \\\\ \\Delta B \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ -2 & 2 \\end{pmatrix} \\begin{pmatrix} \\Delta x \\\\ \\Delta y \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06efaaae-dc50-4006-950f-610aa60389b9",
   "metadata": {},
   "source": [
    "## Shortcut\n",
    "\n",
    "### The previous step-by-step algorithm for linearization is correct, but there is a shortcut to find the linearization systematically. Suppose we want to linearize a transformation $x, y \\implies A,B$  of the form\n",
    "\n",
    "## $$ A = A(x, y) $$\n",
    "## $$ B = B(x, y) $$\n",
    "\n",
    "### at the point $(x_0, y_0)$\n",
    "\n",
    "## Theorem\n",
    "\n",
    "### The linearization of $x, y \\implies A, B$ at $(x_0, y_0)$ is given by the matrix:\n",
    "## $$ \\begin{pmatrix} A_x & A_y \\\\ B_x & B_y \\end{pmatrix}\\Bigg\\rvert_{(x, y) = (x_0, y_0)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46964a86-12b1-4c27-ba40-784b40869472",
   "metadata": {},
   "source": [
    "## Why does the shortcut work?\n",
    "\n",
    "### The shortcut works because the general formula for linear approximation says that\n",
    "\n",
    "## $$ A(x_0 + \\Delta x, y_0 + \\Delta y) - A(x_0, y_0) \\approx A_x(x_0, y_0) \\Delta x + A_y(x_0, y_0) \\Delta y $$\n",
    "## $$ B(x_0 + \\Delta x, y_0 + \\Delta y) - B(x_0, y_0) \\approx B_x(x_0, y_0) \\Delta x + B_y(x_0, y_0) \\Delta y $$\n",
    "\n",
    "### Looking carefully, we can extract the hidden matrix:\n",
    "## $$ \\begin{pmatrix} A(x_0 + \\Delta x, y_0 + \\Delta y) - A(x_0, y_0) \\\\ B(x_0 + \\Delta x, y_0 + \\Delta y) - B(x_0, y_0) \\end{pmatrix} \\approx \\underbrace{\\begin{pmatrix} A_x(x_0, y_0) & A_y(x_0, y_0) \\\\ B_x(x_0, y_0) & B_y(x_0, y_0) \\end{pmatrix}}_{\\text{Linearization}} \\begin{pmatrix} \\Delta x \\\\ \\Delta y \\end{pmatrix} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95beec-f3d1-4a3c-bf34-f36441c53b9e",
   "metadata": {},
   "source": [
    "## The Jacobian Matrix\n",
    "\n",
    "### The matrix appearing in the above Theorem is called the â€œJacobian matrix\".\n",
    "\n",
    "### The Jacobian matrix of the transformation $x, y \\implies A, B$ at the point $(x_0, y_0)$ is the matrix:\n",
    "## $$ \\begin{pmatrix} A_x & A_y \\\\ B_x & B_y \\end{pmatrix}\\Bigg\\rvert_{(x, y) = (x_0, y_0)} $$\n",
    "\n",
    "### In a phrase, the Jacobian is the hidden matrix that drives linear approximation. When you hear â€œJacobian\" think of â€œlinear approximation matrix.\"\n",
    "\n",
    "### The input to Jacobian $J$ should be a small change in the input, $\\Delta x, \\Delta y$ and the output is the approximate change in the output, $\\Delta A, \\Delta B$. In symbols:\n",
    "\n",
    "## $$ J \\begin{pmatrix} \\Delta x \\\\ \\Delta y \\end{pmatrix} \\approx \\begin{pmatrix} \\Delta A \\\\ \\Delta B \\end{pmatrix} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
